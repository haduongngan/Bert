{"cells":[{"cell_type":"markdown","metadata":{"id":"5zqn-9fMUMrc"},"source":["## Install dependences\n","- pytorch-lightning: a simple trainer to help you minize code base\n","- transformers: library contains multiple BERT models\n","- sentencepiece: a word-to-vect library with fast implementation"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25378,"status":"ok","timestamp":1636558209396,"user":{"displayName":"Hà Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjM8w7rbXWqAVegT56_EDIeblRMWvDtxCQ-R_ha=s64","userId":"12708187544018747563"},"user_tz":-420},"id":"9uVlPq0rUeIO","outputId":"7c0e2a99-6e12-4e38-a525-4a48922e9b3b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pytorch-lightning\n","  Downloading pytorch_lightning-1.5.1-py3-none-any.whl (1.0 MB)\n","\u001b[K     |████████████████████████████████| 1.0 MB 4.2 MB/s \n","\u001b[?25hRequirement already satisfied: tensorboard\u003e=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.7.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (3.10.0.2)\n","Requirement already satisfied: numpy\u003e=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.19.5)\n","Collecting pyDeprecate==0.3.1\n","  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n","Requirement already satisfied: tqdm\u003e=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.62.3)\n","Collecting future\u003e=0.17.1\n","  Downloading future-0.18.2.tar.gz (829 kB)\n","\u001b[K     |████████████████████████████████| 829 kB 38.3 MB/s \n","\u001b[?25hCollecting PyYAML\u003e=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 40.1 MB/s \n","\u001b[?25hCollecting fsspec[http]!=2021.06.0,\u003e=2021.05.0\n","  Downloading fsspec-2021.11.0-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 49.4 MB/s \n","\u001b[?25hRequirement already satisfied: packaging\u003e=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.2)\n","Collecting torchmetrics\u003e=0.4.1\n","  Downloading torchmetrics-0.6.0-py3-none-any.whl (329 kB)\n","\u001b[K     |████████████████████████████████| 329 kB 46.2 MB/s \n","\u001b[?25hRequirement already satisfied: torch\u003e=1.6 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.9.0+cu111)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 33.1 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,\u003e=2021.05.0-\u003epytorch-lightning) (2.23.0)\n","Requirement already satisfied: pyparsing\u003c3,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=17.0-\u003epytorch-lightning) (2.4.7)\n","Requirement already satisfied: tensorboard-plugin-wit\u003e=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning) (1.8.0)\n","Requirement already satisfied: wheel\u003e=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning) (0.37.0)\n","Requirement already satisfied: protobuf\u003e=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning) (3.17.3)\n","Requirement already satisfied: setuptools\u003e=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning) (57.4.0)\n","Requirement already satisfied: tensorboard-data-server\u003c0.7.0,\u003e=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning) (0.6.1)\n","Requirement already satisfied: werkzeug\u003e=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning) (1.0.1)\n","Requirement already satisfied: absl-py\u003e=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning) (0.12.0)\n","Requirement already satisfied: grpcio\u003e=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning) (1.41.1)\n","Requirement already satisfied: google-auth\u003c3,\u003e=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning) (1.35.0)\n","Requirement already satisfied: google-auth-oauthlib\u003c0.5,\u003e=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning) (0.4.6)\n","Requirement already satisfied: markdown\u003e=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning) (3.3.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py\u003e=0.4-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning) (1.15.0)\n","Requirement already satisfied: pyasn1-modules\u003e=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning) (0.2.8)\n","Requirement already satisfied: rsa\u003c5,\u003e=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning) (4.7.2)\n","Requirement already satisfied: cachetools\u003c5.0,\u003e=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning) (4.2.4)\n","Requirement already satisfied: requests-oauthlib\u003e=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib\u003c0.5,\u003e=0.4.1-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown\u003e=2.6.8-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning) (4.8.1)\n","Requirement already satisfied: pyasn1\u003c0.5.0,\u003e=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules\u003e=0.2.1-\u003egoogle-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning) (0.4.8)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003efsspec[http]!=2021.06.0,\u003e=2021.05.0-\u003epytorch-lightning) (3.0.4)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003efsspec[http]!=2021.06.0,\u003e=2021.05.0-\u003epytorch-lightning) (2.10)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003efsspec[http]!=2021.06.0,\u003e=2021.05.0-\u003epytorch-lightning) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003efsspec[http]!=2021.06.0,\u003e=2021.05.0-\u003epytorch-lightning) (1.24.3)\n","Requirement already satisfied: oauthlib\u003e=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib\u003e=0.7.0-\u003egoogle-auth-oauthlib\u003c0.5,\u003e=0.4.1-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning) (3.1.1)\n","Collecting aiosignal\u003e=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting multidict\u003c7.0,\u003e=4.5\n","  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n","\u001b[K     |████████████████████████████████| 160 kB 48.6 MB/s \n","\u001b[?25hRequirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]!=2021.06.0,\u003e=2021.05.0-\u003epytorch-lightning) (21.2.0)\n","Requirement already satisfied: charset-normalizer\u003c3.0,\u003e=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]!=2021.06.0,\u003e=2021.05.0-\u003epytorch-lightning) (2.0.7)\n","Collecting async-timeout\u003c5.0,\u003e=4.0.0a3\n","  Downloading async_timeout-4.0.0-py3-none-any.whl (6.1 kB)\n","Collecting frozenlist\u003e=1.1.1\n","  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n","\u001b[K     |████████████████████████████████| 192 kB 49.6 MB/s \n","\u001b[?25hCollecting yarl\u003c2.0,\u003e=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 55.0 MB/s \n","\u001b[?25hCollecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003emarkdown\u003e=2.6.8-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning) (3.6.0)\n","Building wheels for collected packages: future\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=701bce46f596c518d13272474e1ad1122ed4668f10df434cf6fb5155a2e74fdc\n","  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n","Successfully built future\n","Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, torchmetrics, PyYAML, pyDeprecate, future, pytorch-lightning\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Attempting uninstall: future\n","    Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","Successfully installed PyYAML-6.0 aiohttp-3.8.0 aiosignal-1.2.0 async-timeout-4.0.0 asynctest-0.13.0 frozenlist-1.2.0 fsspec-2021.11.0 future-0.18.2 multidict-5.2.0 pyDeprecate-0.3.1 pytorch-lightning-1.5.1 torchmetrics-0.6.0 yarl-1.7.2\n","Collecting transformers\n","  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 4.2 MB/s \n","\u001b[?25hRequirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub\u003c1.0,\u003e=0.1.0\n","  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n","\u001b[K     |████████████████████████████████| 59 kB 6.6 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.2)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Collecting tokenizers\u003c0.11,\u003e=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 35.8 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 47.2 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.1.0-\u003etransformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing\u003c3,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.0-\u003etransformers) (2.4.7)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003etransformers) (3.6.0)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (3.0.4)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (1.24.3)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers) (7.1.2)\n","Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.1.2 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.3\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 4.3 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n"]}],"source":["!pip install pytorch-lightning\n","!pip install transformers\n","!pip install sentencepiece"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50485,"status":"ok","timestamp":1636558259867,"user":{"displayName":"Hà Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjM8w7rbXWqAVegT56_EDIeblRMWvDtxCQ-R_ha=s64","userId":"12708187544018747563"},"user_tz":-420},"id":"PCDyJwo0UnIk","outputId":"bd2a109a-0c1d-4350-e885-4d8a1048ddef"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# mount to your drive and access your dataset\n","from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"markdown","metadata":{"id":"ObVSjGJ2Adwi"},"source":["# LOAD DATA"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":84,"status":"ok","timestamp":1636558259869,"user":{"displayName":"Hà Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjM8w7rbXWqAVegT56_EDIeblRMWvDtxCQ-R_ha=s64","userId":"12708187544018747563"},"user_tz":-420},"id":"C5tPeRinVUmh","outputId":"d7d839ee-f1ad-4049-a65f-fc8f030bf0d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["ls: cannot access '/content/drive/MyDrive/Colab/BERT/shopee-sentiment': No such file or directory\n"]}],"source":["# replace this path to your dataset directory\n","DATA_ROOT_DIR=\"/content/drive/MyDrive/Colab/BERT/shopee-sentiment\"\n","!ls $DATA_ROOT_DIR"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"T-95Em2MAchd"},"outputs":[],"source":["# include some dependence\n","import pandas as pd\n","import numpy as np\n","from torch.utils.data import random_split, DataLoader, Dataset\n","import pytorch_lightning as pl\n","import torch.nn as nn\n","import torch\n","\n","train_ratio = 0.8\n","DATA_DIR = '/content/drive/MyDrive/Colab/BERT/preprocess_data.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"pk6fOTGRhDUb"},"outputs":[{"ename":"FileNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-6-3b5e1ecaae06\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use pandas to read csv, this will return a excel like table data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab/BERT/preprocess_data.csv'"]}],"source":["# Use pandas to read csv, this will return a excel like table data\n","train = pd.read_csv(DATA_DIR,usecols=['text', 'class']).dropna()\n","train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"PzegOvuxhla8"},"outputs":[],"source":["from typing import Optional\n","class SentimentData(Dataset):\n","    \"\"\"\n","    Dataset class for sentiment analysis. \n","    Every dataset using pytorch should be overwrite this class\n","    This require 2 function, __len__ and __getitem__\n","    \"\"\"\n","    def __init__(self, data_dir):\n","        \"\"\"\n","        Args:\n","            data_dir (string): Directory with the csv file\n","        \"\"\"\n","        self.df = pd.read_csv(data_dir, index_col=0)\n","\n","    def __len__(self):\n","        \"\"\"\n","        length of the dataset, i.e. number of rows in the csv file\n","        Returns: int \n","        \"\"\"\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        given a row index, returns the corresponding row of the csv file\n","        Returns: text (string), label (int) \n","        \"\"\"\n","        text = self.df[\"text\"][idx]\n","        label = self.df[\"class\"][idx]\n","\n","        return text, label\n","\n","\n","class SentimentDataModule(pl.LightningDataModule):\n","    \"\"\"\n","    Module class for sentiment analysis. this class is used to load the data to the model. \n","    It is a subclass of LightningDataModule. \n","    \"\"\"\n","\n","    def __init__(self, data_dir: str = DATA_DIR, batch_size: int = 16):\n","        \"\"\"\n","        Args:\n","            data_dir (string): Directory with the csv file\n","            batch_size (int): batch size for dataloader\n","        \"\"\"\n","        super().__init__()\n","        self.data_dir = data_dir\n","        self.batch_size = batch_size\n","\n","    def setup(self, stage: Optional[str] = None):\n","        \"\"\"\n","        Loads the data to the model. \n","        the data is loaded in the setup function, so that it is loaded only once. \n","        \"\"\"\n","        data_full = SentimentData(self.data_dir)\n","        train_size = round(len(data_full) * train_ratio)\n","        val_size = len(data_full) - train_size\n","        print(len(data_full), train_size, val_size)\n","        self.data_train, self.data_val = random_split(data_full, [train_size, val_size])\n","\n","    def train_dataloader(self):\n","        \"\"\"\n","        Returns: dataloader for training\n","        \"\"\"\n","        return DataLoader(self.data_train, batch_size=self.batch_size)\n","\n","    def val_dataloader(self):\n","        \"\"\"\n","        Returns: dataloader for validation\n","        \"\"\"\n","        return DataLoader(self.data_val, batch_size=self.batch_size)\n","\n","# Do some Test with data\n","if __name__ == \"__main__\":\n","\tdm = SentimentDataModule(DATA_DIR)\n","\tdm.setup()\n","\tidx = 0\n","\tfor item in (dm.train_dataloader()):\n","\t\tprint(idx)\n","\t\tprint(item)\n","\t\tidx += 1\n","\t\tif idx \u003e 5: break\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jJc7Ez9lFxQl"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"gJjb04m0nGBX"},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wNJEoE4LblV-"},"outputs":[],"source":["!pip install fairseq"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"laCD7EF5PZnz"},"outputs":[],"source":["from fairseq.data import Dictionary\n","import sentencepiece as spm\n","from os.path import join as pjoin\n","from transformers import PreTrainedTokenizer\n","import sentencepiece as spm\n","\n","\n","class XLMRobertaTokenizer(PreTrainedTokenizer):\n","    \"\"\"\n","    XLM-RoBERTa tokenizer adapted from transformers.PreTrainedTokenizer. This helps to convert the input text into \n","    tokenized format. eg, \n","    \n","    input: \"Hello, how are you?\" output: [\"1\", \"2\", \"3\", \"65\", \"2\", \"1\"]\n","    \n","    this class also provides the method to convert the tokenized format into the original text.\n","    \n","    eg, input: [\"1\", \"2\", \"3\", \"65\", \"2\", \"1\"] output: \"Hello, how are you?\"\n","    \n","    \"\"\"\n","    def __init__(\n","            self,\n","            pretrained_file,\n","            bos_token=\"\u003cs\u003e\",\n","            eos_token=\"\u003c/s\u003e\",\n","            sep_token=\"\u003c/s\u003e\",\n","            cls_token=\"\u003cs\u003e\",\n","            unk_token=\"\u003cunk\u003e\",\n","            pad_token=\"\u003cpad\u003e\",\n","            mask_token=\"\u003cmask\u003e\",\n","            **kwargs\n","    ):\n","        \"\"\"\n","        :param pretrained_file: path to the pretrained model file\n","        :param bos_token: beginning of sentence token\n","        :param eos_token: end of sentence token\n","        :param sep_token: separation token\n","        :param cls_token: classification token\n","        :param unk_token: unknown token\n","        :param pad_token: padding token\n","        :param mask_token: mask token\n","        \"\"\"\n","        super().__init__(\n","            bos_token=bos_token,\n","            eos_token=eos_token,\n","            unk_token=unk_token,\n","            sep_token=sep_token,\n","            cls_token=cls_token,\n","            pad_token=pad_token,\n","            mask_token=mask_token,\n","            **kwargs,\n","        )\n","        # load bpe model and vocab file\n","        sentencepiece_model = pjoin(pretrained_file, 'sentencepiece.bpe.model')\n","        vocab_file = pjoin(pretrained_file, 'dict.txt')\n","        self.sp_model = spm.SentencePieceProcessor()\n","        self.sp_model.Load(\n","            sentencepiece_model)  # please dont use anything from sp_model bcz it makes everything goes wrong\n","        self.bpe_dict = Dictionary().load(vocab_file)\n","        # Mimic fairseq token-to-id alignment for the first 4 token\n","        self.fairseq_tokens_to_ids = {\"\u003cs\u003e\": 0, \"\u003cpad\u003e\": 1, \"\u003c/s\u003e\": 2, \"\u003cunk\u003e\": 3}\n","        # The first \"real\" token \",\" has position 4 in the original fairseq vocab and position 3 in the spm vocab\n","        self.fairseq_offset = 0\n","        self.fairseq_tokens_to_ids[\"\u003cmask\u003e\"] = len(self.bpe_dict) + self.fairseq_offset\n","        self.fairseq_ids_to_tokens = {v: k for k, v in self.fairseq_tokens_to_ids.items()}\n","\n","    def _tokenize(self, text):\n","        \"\"\" Tokenize a string. \"\"\"\n","        return self.sp_model.EncodeAsPieces(text)\n","\n","    def _convert_token_to_id(self, token):\n","        \"\"\" Converts a token (str) in an id using the vocab. \"\"\"\n","        if token in self.fairseq_tokens_to_ids:\n","            return self.fairseq_tokens_to_ids[token]\n","        spm_id = self.bpe_dict.index(token)\n","        return spm_id\n","\n","    def _convert_id_to_token(self, index):\n","        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n","        if index in self.fairseq_ids_to_tokens:\n","            return self.fairseq_ids_to_tokens[index]\n","        return self.bpe_dict[index]\n","\n","    @property\n","    def vocab_size(self):\n","        \"\"\" Size of the base vocabulary (without the added tokens) \"\"\"\n","        return len(self.bpe_dict) + self.fairseq_offset + 1  # Add the \u003cmask\u003e token\n","\n","    def get_vocab(self):\n","        \"\"\" Returns the vocabulary as a list of tokens. \"\"\"\n","        vocab = {self.convert_ids_to_tokens(i): i for i in range(self.vocab_size)}\n","        vocab.update(self.added_tokens_encoder)\n","        return vocab"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"sypO4qMvnwOL"},"outputs":[],"source":["from transformers import XLMRobertaConfig, XLMRobertaForSequenceClassification\n","import torch\n","\n","pretrained_path = '/content/drive/MyDrive/Colab/BERT/envibert/'\n","!ls $pretrained_path\n","# load tokenizer\n","roberta = XLMRobertaForSequenceClassification.from_pretrained(pretrained_path)\n","tokenizer = XLMRobertaTokenizer(pretrained_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"g8mZiHH8yltU"},"outputs":[],"source":["# try to convert some text into numbers\n","inputs = [\"Tôi ghét nó\", \"Tôi thích nó\", \"Tôi quý nó\"]\n","inputs = tokenizer(inputs, return_tensors='pt')\n","print(inputs)\n","outputs = roberta(**inputs, labels=torch.tensor([0, 1, 1]))\n","print(outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"GW03BijknFFb"},"outputs":[],"source":["from sklearn.metrics import roc_auc_score, classification_report, accuracy_score\n","\n","\n","class SentimentRoberta(pl.LightningModule):\n","    \"\"\"\n","    SentimentRoberta class inherits from LightningModule\n","    This class is used to train a model using PyTorch Lightning\n","    It overrides the following methods:\n","        - forward : forward pass of the model\n","        - training_step : training step of the model\n","        - validation_step : validation step of the model\n","        - validation_epoch_end : end of the validation epoch\n","        - configure_optimizers : configure optimizers\n","    \"\"\"\n","    def __init__(self, lr_roberta, lr_classifier):\n","        \"\"\"\n","        Initialize the model with the following parameters:\n","            - lr_roberta : learning rate of the roberta model\n","            - lr_classifier : learning rate of the classifier model\n","        \"\"\"\n","        super().__init__()\n","        self.roberta = XLMRobertaForSequenceClassification.from_pretrained(pretrained_path)\n","        self.tokenizer = XLMRobertaTokenizer(pretrained_path)\n","        self.lr_roberta = lr_roberta\n","        self.lr_classifer = lr_classifier\n","\n","    def forward(self, texts, labels=None):\n","        \"\"\"\n","        Forward pass of the model\n","        Args:\n","            - texts : input texts\n","            - labels : labels of the input texts\n","        \"\"\"\n","        inputs = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=256)\n","        for key in inputs:\n","            inputs[key] = inputs[key].to(self.device)\n","\n","        outputs = self.roberta(**inputs, labels=labels)\n","        return outputs\n","\n","    def configure_optimizers(self):\n","        \"\"\"\n","        Configure optimizers\n","        This method is used to configure the optimizers of the model by using the learning rate\n","        for specific parameter of the roberta model and the classifier model\n","        \"\"\"\n","        roberta_params = self.roberta.roberta.named_parameters()\n","        classifier_params = self.roberta.classifier.named_parameters()\n","\n","        grouped_params = [\n","            {\"params\": [p for n, p in roberta_params], \"lr\": self.lr_roberta},\n","            {\"params\": [p for n, p in classifier_params], \"lr\": self.lr_classifer}\n","        ]\n","        optimizer = torch.optim.AdamW(\n","            grouped_params\n","        )\n","        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.98)\n","        return {\n","            'optimizer': optimizer,\n","            'lr_scheduler': {\n","                'scheduler': scheduler,\n","                'monitor': 'f1/val',\n","            }\n","        }\n","\n","    def training_step(self, batch, batch_idx):\n","        \"\"\"\n","        Training step of the model\n","        Args:\n","            - batch : batch of the data\n","            - batch_idx : index of the batch\n","        \"\"\"\n","        texts, labels = batch\n","        outputs = self(texts, labels=labels)\n","\n","        if len(outputs.values()) == 3:\n","            loss, logits, _ = outputs.values()\n","        else:\n","            loss, logits = outputs.values()\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        \"\"\"\n","        Validation step of the model, used to compute the metrics\n","        Args:\n","            - batch : batch of the data\n","            - batch_idx : index of the batch\n","        \"\"\"\n","        texts, labels = batch\n","        outputs = self(texts, labels=labels)\n","\n","        if len(outputs.values()) == 3:\n","            loss, logits, _ = outputs.values()\n","        else:\n","            loss, logits = outputs.values()\n","\n","        output_scores = torch.softmax(logits, dim=-1)\n","        return loss, output_scores, labels\n","\n","    def validation_epoch_end(self, validation_step_outputs):\n","        \"\"\"\n","        End of the validation epoch, this method will be called at the end of the validation epoch,\n","        it will compute the multiple metrics of classification problem\n","        Args:\n","            - validation_step_outputs : outputs of the validation step\n","        \"\"\"\n","\n","        val_preds = torch.tensor([], device=self.device)\n","        val_scores = torch.tensor([], device=self.device)\n","        val_labels = torch.tensor([], device=self.device)\n","        val_loss = 0\n","        total_item = 0\n","\n","        for idx, item in enumerate(validation_step_outputs):\n","            loss, output_scores, labels = item\n","\n","            predictions = torch.argmax(output_scores, dim=-1)\n","            val_preds = torch.cat((val_preds, predictions), dim=0)\n","            val_scores = torch.cat((val_scores, output_scores[:, 1]), dim=0)\n","            val_labels = torch.cat((val_labels, labels), dim=0)\n","\n","            val_loss += loss\n","            total_item += 1\n","\n","        # print(\"VAL PREDS\", val_preds.shape)\n","        # print(\"VAL SCORES\", val_scores.shape)\n","        # print(\"VAL LABELS\", val_labels.shape)\n","        val_preds = val_preds.cpu().numpy()\n","        val_scores = val_scores.cpu().numpy()\n","        val_labels = val_labels.cpu().numpy()\n","\n","        reports = classification_report(val_labels, val_preds, output_dict=True)\n","        print(\"VAL LABELS\", val_labels)\n","        print(\"VAL SCORES\", val_scores)\n","        try:\n","            auc = roc_auc_score(val_labels, val_scores)\n","        except Exception as e:\n","            print(e)\n","            print(\"Cannot calculate AUC. Default to 0\")\n","            auc = 0\n","        accuracy = accuracy_score(val_labels, val_preds)\n","\n","        print(classification_report(val_labels, val_preds))\n","\n","        self.log(\"loss/val\", val_loss)\n","        self.log(\"auc/val\", auc)\n","        self.log(\"accuracy/val\", accuracy)\n","        self.log(\"precision/val\", reports[\"weighted avg\"][\"precision\"])\n","        self.log(\"recall/val\", reports[\"weighted avg\"][\"recall\"])\n","        self.log(\"f1/val\", reports[\"weighted avg\"][\"f1-score\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qMRyo5iYBvn8"},"outputs":[],"source":["\n","trainer = pl.Trainer(\n","    fast_dev_run=True,\n",")\n","model = SentimentRoberta(lr_roberta=1e-5, lr_classifier=3e-3)\n","dm = SentimentDataModule()\n","\n","trainer.fit(model, dm)"]},{"cell_type":"markdown","metadata":{"id":"0HtXPhT4g9VN"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BfhWop6-hD08"},"outputs":[],"source":["from pytorch_lightning import loggers as pl_loggers\n","from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","torch.manual_seed(123)\n","\n","tb_logger = pl_loggers.TensorBoardLogger('/content/drive/MyDrive/colab/BERT/logs/')\n","\n","trainer = pl.Trainer(\n","    min_epochs=1,\n","    max_epochs=5,\n","    gpus=1,\n","    precision=16,\n","    val_check_interval=0.5,\n","    # check_val_every_n_epoch=1,\n","    callbacks=[\n","      ModelCheckpoint(\n","          dirpath='/content/drive/MyDrive/colab/BERT/ckpt',\n","          save_top_k=3,\n","          monitor='f1/val',\n","      ), \n","      EarlyStopping('f1/val', patience=5)\n","    ],\n","    fast_dev_run=False,\n","    logger=tb_logger\n",")\n","\n","dm.setup(stage=\"fit\")\n","trainer.fit(model, dm)"]},{"cell_type":"markdown","metadata":{"id":"L8wvJNe5xG62"},"source":["TEST"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Qo1gSkpVxO80"},"outputs":[],"source":["# show the result here\n","%reload_ext tensorboard\n","%tensorboard --logdir '/content/drive/MyDrive/colab/BERT/logs/'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"QKVQrC1TkzRG"},"outputs":[],"source":["# test the model with some sentence\n","inputs = [\"Món này ăn chán vậy\"]\n","outputs = model(inputs)\n","logits = outputs['logits']\n","score = torch.softmax(logits, dim=-1)\n","Labels = [\"Positive\", \"Negative\"]\n","print(score)\n","print(f\"The sentence: '{inputs[0]}' has {Labels[torch.argmax(score, dim=-1).item())]} tone with confident score : {score[torch.argmax(score, dim=-1).item())]}\" )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"KYfEvaGJqLo7"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"RoBERTa-Sentiment.ipynb","provenance":[{"file_id":"1Lsh3mlmybwoaOSA8hCnvVC2BOIs-TqxF","timestamp":1636516888267},{"file_id":"1wdt7z8UcDla3EAjJXI-pxmqZEEB3ry4Z","timestamp":1635951666520},{"file_id":"1bBnyzCkj2Imdk5hrZ0OAOH1l8aDLF4YW","timestamp":1635433549658}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}